Metadata-Version: 2.4
Name: rainbowplus
Version: 0.1.0
Summary: RainbowPlus: Enhancing Adversarial Prompt Generation via Evolutionary Quality-Diversity Search
Home-page: https://github.com/knoveleng/rainbowplus
Author: Knovel Engineering Lab
Author-email: andrew.dang@knoveleng.com
License: MIT
Project-URL: Bug Reports, https://github.com/knoveleng/rainbowplus/issues
Project-URL: Source, https://github.com/knoveleng/rainbowplus
Keywords: llm,language-model,evaluation,robustness,ai-safety
Classifier: Development Status :: 4 - Beta
Classifier: Intended Audience :: Science/Research
Classifier: License :: OSI Approved :: MIT License
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.8
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Requires-Python: >=3.8
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: vllm>=0.6.3
Requires-Dist: torch>=2.6.0
Requires-Dist: openai>=1.0.0
Requires-Dist: nltk>=3.8.1
Requires-Dist: pydantic>=2.0.0
Requires-Dist: PyYAML>=6.0
Requires-Dist: datasets>=2.14.0
Requires-Dist: tenacity>=8.0.0
Requires-Dist: httpx>=0.24.0
Requires-Dist: sentence-transformers>=2.2.0
Requires-Dist: scikit-learn>=1.0.0
Requires-Dist: nltk>=3.8.1
Requires-Dist: sentence-transformers>=2.2.0
Requires-Dist: transformers<4.54.0
Requires-Dist: matplotlib>=3.5.0
Requires-Dist: seaborn>=0.11.0
Requires-Dist: lexicalrichness>=0.3.0
Requires-Dist: wordcloud>=1.8.2
Requires-Dist: python-dotenv>=0.19.0
Provides-Extra: dev
Requires-Dist: pytest>=7.0.0; extra == "dev"
Requires-Dist: black>=23.0.0; extra == "dev"
Requires-Dist: isort>=5.12.0; extra == "dev"
Requires-Dist: flake8>=6.0.0; extra == "dev"
Dynamic: author
Dynamic: author-email
Dynamic: classifier
Dynamic: description
Dynamic: description-content-type
Dynamic: home-page
Dynamic: keywords
Dynamic: license
Dynamic: license-file
Dynamic: project-url
Dynamic: provides-extra
Dynamic: requires-dist
Dynamic: requires-python
Dynamic: summary

# ğŸ¤–ğŸ”„ğŸ§‘ PersonaTeaming

## ğŸ“‹ Overview
This repository contains the implementation of the methods described in our workshop paper **"[PersonaTeaming: Exploring How Introducing Personas Can Improve Automated AI Red-Teaming](https://arxiv.org/abs/2509.03728)"**
based on the codebase from **"[RainbowPlus: Enhancing Adversarial Prompt Generation via Evolutionary Quality-Diversity Search](https://arxiv.org/abs/2504.15047)"**. 

Prior work such as **RainbowTeaming** and **RainbowPlus** introduces algorithm to leverage evolutionary quality-diversity (QD) paradigm to mutate a set of seed prompts based on dimentions such as risk categories and attack style. While automated red-teaming approaches promise to complement human red-teaming by enabling larger-scale exploration of model behavior, current approaches do not consider the role of identity. 

As an initial step towards incorporating people's background and identities in automated red-teaming, we develop and evaluate a novel method, **Personateaming**, that introduces personas in the adversarial prompt generation process to explore a wider spectrum of adversarial strategies.

In particular, we first introduce a methodology for mutating prompts based on either "red-teaming expert" personas or "regular AI user" personas. We then develop a dynamic persona-generating algorithm that automatically generates various persona types adaptive to different seed prompts. In addition, we develop a set of new metrics to explicitly measure the "mutation distance" to complement existing diversity measurements of adversarial prompts.

![Diagram](/assets/diagram.png)

Throuh a preliminary experiment, we found promising improvements (up to 144.1\%) in the attack success rates of adversarial prompts through persona mutation, while maintaining prompt diversity, compared to **RainbowPlus**, a state-of-the-art automated red-teaming method. Please read our **"[Workshop Paper](https://arxiv.org/abs/2509.03728)"** for more detailed explaination!

![Results](/assets/preliminary-result.png)

## ğŸ“ Repository Structure

Note that this strucutre is an extension of RainbowPlus codebase https://github.com/knoveleng/rainbowplus

```
â”œâ”€â”€ configs/                  # Configuration files
â”‚   â”œâ”€â”€ categories/           # Category definitions
â”‚   â”œâ”€â”€ styles/               # Style definitions
â”‚   â”œâ”€â”€ base.yml              # Base configuration
â”‚   â”œâ”€â”€ base-openai.yml       # Configuration to run LLMs from OpenAI
â”‚   â”œâ”€â”€ base-opensource.yml   # Configuration to run open-source LLMs
â”‚   â””â”€â”€ eval.yml              # Evaluation configuration
â”‚
â”œâ”€â”€ data/                     # Dataset storage
â”‚
â”œâ”€â”€ rainbowplus/              # Core package
â”‚   â”œâ”€â”€ configs/              # Configuration utilities
â”‚   â”œâ”€â”€ llms/                 # LLM integration modules
â”‚   â”œâ”€â”€ scores/               # Fitness and similarity functions
â”‚   â”œâ”€â”€ mutators/             # Persona mutator functions
â”‚   â”œâ”€â”€ archive.py            # Archive management
â”‚   â”œâ”€â”€ evaluate.py           # Current evaluation implementation
â”‚   â”œâ”€â”€ evaluate_v0.py        # Evaluation implementation from old version
â”‚   â”œâ”€â”€ get_scores.py         # Metrics extraction utilities
â”‚   â”œâ”€â”€ prompts.py            # LLM prompt templates
â”‚   â”œâ”€â”€ rainbowplus.py        # Main implementation
â”‚   â””â”€â”€ utils.py              # Utility functions
â”‚
â”œâ”€â”€ sh/                       # Shell scripts
â”‚   â””â”€â”€ run.sh                # All-in-one execution script
â”‚
â”œâ”€â”€ README.md                 # This documentation
â””â”€â”€ setup.py                  # Package installation script
```

## ğŸš€ Getting Started

### 1ï¸âƒ£ Environment Setup

Create and activate a Python virtual environment, then install the required dependencies:

```bash
python -m venv venv
source venv/bin/activate
pip install -e .
```

### 2ï¸âƒ£ API Configuration

#### ğŸ¤— Hugging Face Token (Optional)

Required for accessing certain resources from the Hugging Face Hub (e.g., Llama Guard):

```bash
export HF_AUTH_TOKEN="YOUR_HF_TOKEN"
```

Alternatively:

```bash
huggingface-cli login --token=YOUR_HF_TOKEN
```

#### ğŸ”‘ OpenAI API Key

Required when using OpenAI models:

```bash
export OPENAI_API_KEY="YOUR_API_KEY"
```

## ğŸ“Š Usage

### ğŸ§  LLM Configuration

Similar to RainbowPlus, PersonaTeaming currently supports two primary LLM integration methods:

#### 1ï¸âƒ£ vLLM (Open-Source Models)

Example configuration for Qwen-2.5-7B-Instruct:

```yaml
target_llm:
  type_: vllm

  model_kwargs:
    model: Qwen/Qwen2.5-7B-Instruct
    trust_remote_code: True
    max_model_len: 2048
    gpu_memory_utilization: 0.5

  sampling_params:
    temperature: 0.6
    top_p: 0.9
    max_tokens: 1024
```

Additional parameters can be specified according to the [vLLM model documentation](https://docs.vllm.ai/en/latest/api/offline_inference/llm.html) and [sampling parameters documentation](https://docs.vllm.ai/en/latest/api/inference_params.html#sampling-parameters).

#### 2ï¸âƒ£ OpenAI API (Closed-Source Models)

Example configuration for GPT-4o-mini:

```yaml
target_llm:
  type_: openai

  model_kwargs:
    model: gpt-4o-mini

  sampling_params:
    temperature: 0.6
    top_p: 0.9
    max_tokens: 1024
```

Additional parameters can be specified according to the [OpenAI API documentation](https://platform.openai.com/docs/api-reference/chat/create).

### ğŸ§ª Running Experiments

Basic execution with default configuration:

```bash
python -m rainbowplus.rainbowplus --config_file configs/{config-file-name}.yml
```

You should customized your experiment in the config file.

All commands to reproduce our published results can be found in `commands.txt`

#### âš™ï¸ Configuration Parameters

| Parameter | Description |
|-----------|-------------|
| `mutation_strategy` | Choose between "rainbowplus" "combined" and "combined-fit"|
| `persona_config` | Persona config file path|
| `persona_type` | Choose between "RegularAIUsers" and "RedTeamingExperts"|
| `target_llm` | Target LLM identifier |
| `num_samples` | Number of initial seed prompts |
| `max_iters` | Maximum number of iteration steps |
| `sim_threshold` | Similarity threshold for prompt mutation |
| `num_mutations` | Number of prompt mutations per iteration |
| `fitness_threshold` | Minimum fitness score to add prompt to archive |
| `log_dir` | Directory for storing logs |
| `dataset` | Dataset path |
| `shuffle` | Whether to shuffle seed prompts |
| `log_interval` | Number of iterations between log saves |


## ğŸ“Š Evaluation

After running experiments, evaluate the results:

```bash

# Calculating ASR and BLEU-based Diversity Score
python analyze_comprehensive_logs.py logs-{experiment-name}/gpt-4o/harmbench  

# Calculating attack embedding based diversity score, TF-IDF analysis, and corresponding visualizations
python run_attack_analysis.py logs-{experiment-name}/gpt-4o/harmbench/comprehensive_log_global.json --output logs-{experiment-name}/gpt-4o/harmbench/attack_analysis

```

### Evaluation Metrics

For metrics used in our current preliminary evaluation, please refer to section 3.1 Metrics in our **"[Workshop Paper](https://arxiv.org/abs/2509.03728)"**ï¼

## ğŸ“ Citation

```
@article{deng2025personateaming,
  title={PersonaTeaming: Exploring How Introducing Personas Can Improve Automated AI Red-Teaming},
  author={Deng, Wesley Hanwen and Kim, Sunnie SY and Jha, Akshita and Holstein, Ken and Eslami, Motahhare and Wilcox, Lauren and Gatys, Leon A},
  journal={arXiv preprint arXiv:2509.03728},
  year={2025}
}
```

